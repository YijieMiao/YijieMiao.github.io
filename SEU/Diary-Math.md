# 我的SEU求学日记

![](/Images\logo\12.jpg)

一些零散的题目和方法整理：
* [一道简单估计的三角代换方法](/notes/math_rubbish/简单估计的三角代换法.pdf)

* [有限测度当中各种收敛方式之间的关系](/notes/math_rubbish/有限测度当中各种收敛方式之间的关系.pdf)

* [Golden-Thompson inequality](/notes/math_rubbish/Golden-Thompson_inequality.pdf)

* [分组拆分估计](/notes\math_rubbish\分组拆分估计.pdf)

* [部分积的上界估计与Gautschi不等式](/notes/math_rubbish/部分积的上界估计与Gautschi不等式.pdf)

* [杂题汇总](/notes/math/Marvelous_Notes.pdf) 这里存放着同学或网友问过的问题，因为刚刚整理，篇幅不多，所以也不打算每个问题单列出来，而是集中到一个pdf文档中(不定期更新：2025.11.1)

* [AI 提问词](/notes/math/AI提问关键词.pdf) 从UP小号菌子那里参考，后续随着进一步使用，或许会更新一份新的提示词。


---
下面补充了一些对AI与数学的看法，总结为
**学习与优化的原理：从度量到方法论**

### 一、Goodhart 定律与代理目标

#### 核心：度量与目标的背离

**Goodhart 定律** (Goodhart, 1975)：
> 当一种度量被用作目标时，它将不再是一个好的度量。

在优化**真实目标**（难以衡量）时，我们使用**代理目标**（易于衡量）。过度优化代理指标会使其实质价值降低。

#### 定律的三种具体表现

| 类型 | 核心问题 | 学习中的例子 | 关键修正措施 |
| :--- | :--- | :--- | :--- |
| **极端型** (过拟合) | **过度适应**训练集，泛化性能急剧下降。 | 缺少对**本质**的思考，机械训练相似题目，导致**缺乏举一反三**的能力。 | 1. **限制优化**幅度，理解方法本质。 2. **多样化训练**，知道方法的**适用性边界**。 |
| **因果型** (相关非因果) | 错把**相关**当成**因果**，优化指标对真实目标无效。 | 认为“读完书的页数”与“数学水平”有必然因果，导致**浅层阅读**和**理解不足**。 | 持续进行**因果分析**，跟踪**真实结果**来验证代理指标的有效性。 |
| **对抗型** (钻空子) | 为最大化指标，利用设计漏洞，**未推动**真实目标实现。 | 只做简单、容易的题目，以最大化**单位时间做题数量**。 | **多维化**指标设计，避免维度过于单一。对钻空子行为进行惩戒。 |

**总结：** 指标是手段，不可本末倒置。必须持续审视并修正代理目标。

### 二、学习中避免“贪心”思维

**贪心算法**在每一步都选择**局部最优**。它能得到全局最优解，必须满足以下两个条件：

1.  **贪心选择性：** 局部最优选择能引向全局最优。
2.  **最优子结构：** 全局最优解能由子问题的最优解构成。

**教训：** 学习和复杂优化问题往往不满足这两个前提。

* **反例：** 背单词时，**“每小时记忆量”**（局部最优）无法构成**“十天后长期记忆量”**（全局最优），因为存在**遗忘**。
* **优选策略：** 采用**“先探索后利用”**。前期投入时间**探索**、理解本质、抽象方法（建立**工具箱**），后期再高效**利用**，以实现全局最优。

### 三、数学中的两种抽象文化

数学家 **Gowers** 将数学抽象分为两类：

#### 1. 对数学对象的抽象 (第一类抽象)

* **核心：** 推广**数学对象和结果**。
* **典型：** **理论的构建者**。
* **目的：** 通过构建“大理论”（如希尔伯特空间）来“**压缩**”海量的原始数据，提高交流效率。
* **评价：** 结果的**深度**和与其他**数学对象**的紧密关联。

#### 2. 对数学方法的抽象 (第二类抽象)

* **核心：** 推广**解决问题的思路和方法**。
* **典型：** **问题的解决者**。
* **关键：** 剥离应用细节，提炼想法的**动机与目的**（如**概率法**、**测度集中法**）。
* **意义：** 揭示了**数学对象上不相关**的问题，在**方法论上**却可以深刻互联。

**当前不足：** 缺乏**严密阐述第二类抽象的数学语言**，导致这种方法论的抽象常常以不够严谨的**人类自然语言**进行交流。

---

### 四、构建深度学习的“数学笔记”

数学笔记的定位是理解的**“图式”**和**“工具箱”**。

#### 笔记的基本原则

1.  **个人化：** 只写自己的**理解、领悟和补充**，不抄书。
2.  **证明性：** 必须**证明**自己的理解。命题需证明；观点需证据/例子。
3.  **迭代与压缩：** 不断修正、融入新经验，并通过**一般化推广**实现信息的**压缩和提炼**。
    * 老子哲理：**“为学日益，为道日损”**（学习新知时做加法，提炼本质时做减法）。

#### 写作策略（如何“写出来”）

| 策略 | 核心思路 | 例子 |
| :--- | :--- | :--- |
| **推广** | 将解决问题的思路延伸到**一般情况**。 | 利用 $\text{Bezout}$ 恒等式将群论中互素条件的解法推广。 |
| **反例** | 思考“如果去掉某个条件**会如何**？” | 构造反例，证明 $\limsup x_n^2$ 的等式为何需要 $x_n \geq 0$。 |
| **替代性思考** | 思考“为何不试试这样做？”以**理解当前解法的优势或局限性**。 | 探究递归序列的极限：为何选择上下极限而非证明单调性。 |
| **背景探究** | 寻找某个结果对应的**更大数学背景**。 | 从 $\lim \sin(n)$ 的初等做法，扩展到数论中序列的性质。 |

---

### 五、生成式 AI 在数学学习中的使用

#### 核心原则：收益与风险管理

**核心原则：**
> 只问 **AI** **擅长回答**的问题。

**容错与验证原则：**
> 仅在回答的**错误容忍度较高**，且**正确性易于被验证**时，才使用 **AI**.

| 适用原则的场景 | 评估结果 |
| :--- | :--- |
| **头脑风暴**（寻求灵感/例子） | **推荐**：容错度高，验证难度低。 |
| **不常见证明/复杂推理** | **不推荐**：数学错误容忍度极低，且非 $\text{LLM}$ 强项。 |
| **编写程序代码** | **推荐**：错误反馈及时（验证难度低），易于调试修正。 |

#### 认知负债 (Cognitive Debt)

将本应由大脑完成的推理、检索和监控工作外包给 $\text{LLM}$，短期看似高效，长期却会侵蚀**注意力、记忆力与元认知控制**。

* **神经科学证据：** 长期使用 $\text{LLM}$ 写作的学生，大脑功能连通性（神经活动）显著降低，且停用工具后**不会立即恢复**。
* **应对：** 真正有效的学习需要**“理想的困难 ($\text{Desirable Difficulty}$)”**。必须先**独立思考**，进行充分的自我推理后，再借助 $\text{AI}$ 辅助。

### 六、完整的数学解题经验

#### 关键缺失：失败的经验

传统训练只强调“某种方法能解决什么问题”，缺乏“某种方法**不能解决什么问题**”的试错经验，导致**过拟合**。

#### 完整的训练应涵盖三种经验

1.  方法**能直接解决**的问题。
2.  方法**不能直接解决**，但通过**变通手段**后可以解决的问题。
3.  方法**根本不能解决**的问题，以及**替代的方案**。

**练习时的反思：**
* **自我批判：** 是我对方法理解太浅（应深化理解）？
* **方法论批判：** 是此方法在此问题中**根本不可行**（应识别本质困难并寻找替代方法）？

**例子：** 求和极限时，若 $\text{Taylor}$ 展开后的**误差余项** $\sum O(\dots)$ 不符合 $o(1)$ 的要求，则必须放弃逐项估计，转向**和的积分估计**等替代方法。










[返回主页](https://yijiemiao.github.io/)
